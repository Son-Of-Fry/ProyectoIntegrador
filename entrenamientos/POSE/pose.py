# ===========================================================
# HUMAN ACTIVITY RECOGNITION - YOLOv11 Pose (Python Script)
# ===========================================================
# Autor: Robotics Galu
# Descripci√≥n:
#   Entrena un modelo YOLOv11 pose (keypoints) para reconocer
#   actividades humanas a partir de un dataset de Roboflow.
#   Incluye entrenamiento, validaci√≥n, inferencia en im√°genes,
#   videos y c√°mara web en tiempo real.
# ===========================================================

import os
import glob
import torch
import cv2
from ultralytics import YOLO
from roboflow import Roboflow


# ===========================================================
# Step 01. Verificar entorno y GPU
# ===========================================================
print("=== Step 01. Verificar aceleraci√≥n ===")

if torch.cuda.is_available():
    device = "cuda"
    print(f"üöÄ GPU CUDA disponible: {torch.cuda.get_device_name(0)}")

elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    device = "mps"
    print("üçé Aceleraci√≥n MPS (Apple Neural Engine / Metal) disponible y activa.")

else:
    device = "cpu"
    print("‚öôÔ∏è Usando CPU ‚Äî el entrenamiento ser√° m√°s lento.")

print(f"‚úÖ Dispositivo seleccionado autom√°ticamente: {device}")


# ===========================================================
# Step 02. Descargar dataset de Roboflow
# ===========================================================
print("\n=== Step 02. Descargar dataset ===")

rf = Roboflow(api_key="hzoVrDHDbdeFNRLoh2Nq")
project = rf.workspace("pose-detection-twxbg").project("human-activity-ce7zu")
version = project.version(2)
dataset = version.download("yolov8")

DATASET_DIR = dataset.location
print("üìÅ Dataset descargado en:", DATASET_DIR)


# ===========================================================
# Step 03. Configurar modelo YOLOv11 pose
# ===========================================================
print("\n=== Step 03. Cargar modelo YOLOv11 pose ===")
model = YOLO("yolo11s-pose.pt")


# ===========================================================
# Step 04. Entrenar modelo
# ===========================================================
print("\n=== Step 04. Entrenar modelo ===")

results = model.train(
    data=f"{DATASET_DIR}/data.yaml",
    task="pose",
    mode="train",
    imgsz=640,
    epochs=50,
    batch=8,
    device=device
)
print("‚úÖ Entrenamiento completado.")


# ===========================================================
# Step 05. Validar modelo
# ===========================================================
print("\n=== Step 05. Validar modelo ===")
metrics = model.val()
print("üìä M√©tricas:", metrics.results_dict)


# ===========================================================
# Step 06. Buscar el modelo best.pt m√°s reciente
# ===========================================================
print("\n=== Step 06. Buscar modelo entrenado ===")

best_models = glob.glob("runs/pose/train*/weights/best.pt")
BEST = max(best_models, key=os.path.getmtime)
print("‚úÖ Modelo m√°s reciente:", BEST)


# ===========================================================
# Step 07. Resultados gr√°ficos (opcional)
# ===========================================================
print("\n=== Step 07. Analizar resultados ===")
train_dir = os.path.dirname(os.path.dirname(BEST))
images_to_show = [
    "confusion_matrix.png",
    "confusion_matrix_normalized.png",
    "PoseP_curve.png",
    "PoseR_curve.png",
    "val_batch1_pred.jpg",
    "val_batch2_pred.jpg"
]
for img in images_to_show:
    path = os.path.join(train_dir, img)
    if os.path.exists(path):
        print("üñºÔ∏è", path)
    else:
        print("‚ùå No encontrado:", img)


# ===========================================================
# Step 08. Inferencia en im√°genes de test
# ===========================================================
print("\n=== Step 08. Inferencia en im√°genes de test ===")

test_path = os.path.join(DATASET_DIR, "test/images")
if os.path.exists(test_path):
    image_files = [os.path.join(test_path, f)
                   for f in os.listdir(test_path)
                   if f.lower().endswith((".jpg", ".jpeg", ".png"))]

    for img in image_files:
        results = model.predict(source=img, save=True, conf=0.25)
    print("‚úÖ Predicciones guardadas en runs/pose/predict/")
else:
    print("‚ö†Ô∏è No se encontr√≥ carpeta test/images.")


# ===========================================================
# Step 09. Mostrar resultados (opcional matplotlib)
# ===========================================================
print("\n=== Step 09. Mostrar predicciones ===")
pred_path = "runs/pose/predict"
if os.path.exists(pred_path):
    print("üì∏ Im√°genes procesadas en:", pred_path)
else:
    print("‚ö†Ô∏è No se encontraron im√°genes predichas.")


# ===========================================================
# Step 10. Inferencia en video
# ===========================================================
print("\n=== Step 10. Inferencia en video ===")

video_path = "video1.mp4"  # reemplaza con tu video local
if os.path.exists(video_path):
    model.predict(source=video_path, save=True, conf=0.25, iou=0.2)
    print("üé¨ Video procesado y guardado en runs/pose/predict/")
else:
    print("‚ö†Ô∏è No se encontr√≥ video1.mp4, omitiendo este paso.")


# ===========================================================
# Step 11. Exportar modelo final
# ===========================================================
print("\n=== Step 11. Exportar modelo ===")

export_path = "HumanActivity_best.pt"
os.system(f'cp "{BEST}" "{export_path}"')
print(f"‚úÖ Modelo exportado como: {export_path}")


# ===========================================================
# Step 12. Prueba en c√°mara web (tiempo real)
# ===========================================================
print("\n=== Step 12. Detecci√≥n en c√°mara web ===")

if device != "cpu": torch.set_default_device(device)
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("‚ùå No se pudo abrir la c√°mara.")
else:
    print("üé• C√°mara abierta. Presiona 'q' para salir.")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model.predict(frame, conf=0.5, verbose=False)
    annotated = results[0].plot()
    cv2.imshow("Human Activity - YOLOv11 Pose", annotated)

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
print("‚úÖ Detecci√≥n en c√°mara finalizada.")